{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43aa2cee",
   "metadata": {},
   "source": [
    "# Workshop: Web, DOCX, and PDF Scraping in Python  \n",
    "### Case Study: KDHE Boil Water Advisories and Orders\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this workshop, we will build a data pipeline to collect public notices from the Kansas Department of Health and Environment (KDHE) website. Our focus is on boil water advisories (BWA) and boil water orders (BWO), which are issued when contamination is confirmed or suspected in a public water system.\n",
    "\n",
    "Government agencies often publish important public health data as web pages, PDFs, and document files rather than as clean machine-readable datasets. Scraping helps us convert these public notices into structured data we can analyze over time.\n",
    "\n",
    "## Why Scraping?\n",
    "\n",
    "Scraping is useful when:\n",
    "- Data is public but not downloadable in a single table\n",
    "- Information is split across many pages/documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be339368",
   "metadata": {},
   "source": [
    "## Project Plan\n",
    "\n",
    "This workshop focuses on building a webscraping data collection workflow for KDHE consumer confidence report docx files.\n",
    "<!-- #BWA/BWO notices. -->\n",
    "\n",
    "### steps\n",
    "\n",
    "\n",
    "We will:\n",
    "1. Understand the layout of the documents to gather data from\n",
    "2. Setup a document scraper in python to collect relevant information\n",
    "3. Extract core fields and parse into a pandas dataframe\n",
    "4. conduct checks on the data frame to ensure clean data\n",
    "5. Save results to a CSV\n",
    "\n",
    "<!-- 1. Scrape a KDHE listing page to collect notice detail URLs\n",
    "2. Parse each detail page for core fields (title, category, posted date, body text)\n",
    "3. Extract additional structured fields from notice text when possible:\n",
    "   - notice type (advisory/order/rescinded)\n",
    "   - affected area\n",
    "   - start/end timing clues\n",
    "   - precaution language\n",
    "   - contamination reason\n",
    "4. Save results to a tidy CSV for analysis -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a528c3",
   "metadata": {},
   "source": [
    "## Core Python libraries\n",
    "\n",
    "- **requests**: download web pages\n",
    "- **BeautifulSoup (bs4)** + **lxml**: parse and navigate HTML\n",
    "- **re** (regex): pattern matching for dates, IDs, and key phrases\n",
    "- **dateparser**: convert date strings to datetime objects\n",
    "- **pandas**: tabular data cleaning and export\n",
    "- **time**: delays between requests\n",
    "- **pypdf / pdfplumber / pymupdf**: parse PDF notices\n",
    "<!-- - **python-docx / docx2txt**: parse Word documents -->\n",
    "<!-- - **waybackpy or CDX API workflow**: historical backfill from the Internet Archive -->\n",
    "\n",
    "### Environment\n",
    "\n",
    "We will work in a conda env and stepwise within Jupyter Notebook so each step is:\n",
    "- explained in markdown,\n",
    "- implemented in code cells,\n",
    "- and validated with intermediate outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed4f8ea",
   "metadata": {},
   "source": [
    "## Overview of the Project Steps\n",
    "\n",
    "We will follow a pipeline approach:\n",
    "\n",
    "1. **Define source URLs and scraping rules**\n",
    "   - listing page URL\n",
    "   - detail-page URL pattern\n",
    "   - request headers and set up scraping settings\n",
    "\n",
    "2. **Fetch and parse listing page**\n",
    "   - collect links\n",
    "   - keep only valid detail-page links\n",
    "   - deduplicate while preserving order\n",
    "\n",
    "3. **Fetch and parse detail pages**\n",
    "   - extract title, metadata (posted/updated), and body text\n",
    "   - normalize fields into a consistent record format\n",
    "\n",
    "4. **Build a structured dataset**\n",
    "   - combine records into a pandas DataFrame\n",
    "   - sort, inspect missing values, and standardize dates\n",
    "\n",
    "5. **Export and validate**\n",
    "   - save CSV\n",
    "   - run basic checks (required columns, duplicate URLs, date check)\n",
    "\n",
    "6. **(Optional) Extend to documents and historical coverage**\n",
    "   - parse linked PDFs/DOCX files where relevant\n",
    "   <!-- - incorporate archived pages for missing historical notices -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e75c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0a1712c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first file: ABBYVILLE-CITY-OF-KS2015512-DOCX.docx\n"
     ]
    }
   ],
   "source": [
    "# Path to where documents have been saved\n",
    "basedir = r\"\\\\resfs.home.ku.edu\\groups_hipaa\\PSYC\\kdsc_ClassData\\KDSC-CDL-Project2\\Data\\Full Set of CCR Doc Files\" \n",
    "subdir = r\"ccrs2025\\kdhe_A_E\"\n",
    "\n",
    "folder = os.path.join(basedir, subdir)\n",
    "\n",
    "doc_files = [file for file in os.listdir(folder) if file.lower().endswith(\".docx\")]\n",
    "\n",
    "print('first file:', doc_files[0])\n",
    "testdoc = \"ABBYVILLE-CITY-OF-KS2015512-DOCX.docx\"\n",
    "doc_path = os.path.join(basedir, subdir, testdoc)\n",
    "doc = Document(doc_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1384fbf",
   "metadata": {},
   "source": [
    "The docx package allows us to open and make edits to a word document all from just python. Since we are only scrapping the data, we want to extract the information into a structured dataframe and **NOT** change the original file. the python-docx will only save if we call the doc.save(doc_path) command, so all we need to do is never use it\n",
    "\n",
    "the doc variable is loaded as a doc object. The object has attributes that contain the text/numbers/tables and we need to parse through to get the information we need\n",
    "\n",
    "doc.paragraphs \n",
    "\n",
    "attrs = dir(doc)\n",
    "print(attrs)\n",
    "\n",
    "What we need to collect\n",
    "1. PWS NAME\n",
    "2. PWS ID\n",
    "3. Testing results\n",
    "3.1. Regulated Contaminants\n",
    "3.2. Lead and Copper\n",
    "3.3. Chlorine/Chloramines Maximum Disenfection Level\n",
    "3.4. SEcondary Contamionants -- Non-Health Based Contaminants - No Federal Maximum contaminant evel (MCL)\n",
    "3.5. Compliance Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69376025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables = doc.tables\n",
    "# print(len(tables)) #note that document object tables can overlap pages (table beings near the bottom and spills into another page) This look like a seperate table (with new headers) but it is still indexed from the first table\n",
    "# print(tables[0].cell(0,0).text) #multiple ways to index cells using their .text objects. cell(0,0)\n",
    "# print(tables[0].rows[0].cells[0].text)\n",
    "# print(len(tables[0].rows)) #gets amount of rows in table\n",
    "# print(len(tables[0].rows[0].cells)) #gets amount of columns in table\n",
    "# print(tables[0].cell(0,0).text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7de6fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<docx.text.paragraph.Paragraph object at 0x000002746B208190>\n",
      "PWS NAME:\tCITY OF ADMIRE\t\t\tPWS ID: KS2011103\n",
      "CITY OF ADMIRE\n",
      "KS2011103\n"
     ]
    }
   ],
   "source": [
    "p0 = paragraphs[3]\n",
    "print(p0)\n",
    "\n",
    "pws_text = p0.text\n",
    "\n",
    "regex_search = re.search( #regex looks for the parts in ( ) between the matching str and whitespace s*\n",
    "    r'pws\\s*name\\s*:\\s*(.*?)\\s+pws\\s*id\\s*:\\s*([A-Za-z0-9\\-]+)',\n",
    "    str(pws_text),\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "print(str(pws_text.strip()))\n",
    "print(regex_search.group(1))\n",
    "print(regex_search.group(2))\n",
    "# print(expected_headers[0].strip().casefold())\n",
    "# print(str(pws_text).strip().casefold().find(expected_headers[0].strip().casefold()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "8453d7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<docx.text.paragraph.Paragraph object at 0x00000274690F1E90>\n",
      "raw   : ''\n",
      "clean : ''\n",
      "{'regulated contaminants': 'BARIUM', 'collection date': '1/24/2024', 'highest value': '0.16', 'range\\n(low/high)': '0.16', 'unit': 'ppm', '': '2', 'mclg': '2', 'typical source': 'Discharge from metal refineries'}\n",
      "{'regulated contaminants': 'CHROMIUM', 'collection date': '1/24/2024', 'highest value': '1.4', 'range\\n(low/high)': '1.4', 'unit': 'ppb', '': '100', 'mclg': '100', 'typical source': 'Discharge from steel and pulp mills'}\n",
      "{'regulated contaminants': 'FLUORIDE', 'collection date': '1/24/2024', 'highest value': '0.49', 'range\\n(low/high)': '0.49', 'unit': 'ppm', '': '4', 'mclg': '4', 'typical source': 'Natural deposits; Water additive which promotes strong teeth.'}\n",
      "{'regulated contaminants': 'NITRATE', 'collection date': '1/24/2024', 'highest value': '8.4', 'range\\n(low/high)': '8 - 8.4', 'unit': 'ppm', '': '10', 'mclg': '10', 'typical source': 'Runoff from fertilizer use'}\n",
      "{'regulated contaminants': 'SELENIUM', 'collection date': '1/24/2024', 'highest value': '1.5', 'range\\n(low/high)': '1.5', 'unit': 'ppb', '': '50', 'mclg': '50', 'typical source': 'Erosion of natural deposits'}\n",
      "<docx.text.paragraph.Paragraph object at 0x000002746FB59F50>\n",
      "raw   : ''\n",
      "clean : ''\n",
      "{'regulated contaminants': 'BARIUM', 'collection date': '4/16/2024', 'highest value': '0.063', 'range\\n(low/high)': '0.063', 'unit': 'ppm', '': '2', 'mclg': '2', 'typical source': 'Discharge from metal refineries'}\n",
      "{'regulated contaminants': 'FLUORIDE', 'collection date': '4/16/2024', 'highest value': '0.81', 'range\\n(low/high)': '0 - 0.81', 'unit': 'ppm', '': '4', 'mclg': '4', 'typical source': 'Natural deposits; Water additive which promotes strong teeth.'}\n",
      "{'regulated contaminants': 'NITRATE', 'collection date': '1/8/2024', 'highest value': '1', 'range\\n(low/high)': '0.93 - 1', 'unit': 'ppm', '': '10', 'mclg': '10', 'typical source': 'Runoff from fertilizer use'}\n",
      "{'regulated contaminants': 'SELENIUM', 'collection date': '4/16/2024', 'highest value': '1.8', 'range\\n(low/high)': '1.8', 'unit': 'ppb', '': '50', 'mclg': '50', 'typical source': 'Erosion of natural deposits'}\n",
      "<docx.text.paragraph.Paragraph object at 0x000002746FB61AD0>\n",
      "raw   : 'Unit'\n",
      "clean : 'Unit'\n",
      "{'regulated contaminants': 'BARIUM', 'collection date': '4/15/2024', 'water system': 'CITY OF EMPORIA', 'highest value': '0.015', 'range\\n(low/high)': '0.015', 'unit': 'ppm', '': '2', 'mclg': '2', 'typical source': 'Discharge from metal refineries'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pws name</th>\n",
       "      <th>pws id</th>\n",
       "      <th>regulated contaminants</th>\n",
       "      <th>collection date</th>\n",
       "      <th>highest value</th>\n",
       "      <th>range\\n(low/high)</th>\n",
       "      <th>unit</th>\n",
       "      <th>mcl</th>\n",
       "      <th>mclg</th>\n",
       "      <th>typical source</th>\n",
       "      <th></th>\n",
       "      <th>water system</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CITY OF ABBYVILLE</td>\n",
       "      <td>KS2015512</td>\n",
       "      <td>BARIUM</td>\n",
       "      <td>1/24/2024</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>ppm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Discharge from metal refineries</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CITY OF ABBYVILLE</td>\n",
       "      <td>KS2015512</td>\n",
       "      <td>CHROMIUM</td>\n",
       "      <td>1/24/2024</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>ppb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>Discharge from steel and pulp mills</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CITY OF ABBYVILLE</td>\n",
       "      <td>KS2015512</td>\n",
       "      <td>FLUORIDE</td>\n",
       "      <td>1/24/2024</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>ppm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>Natural deposits; Water additive which promote...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CITY OF ABBYVILLE</td>\n",
       "      <td>KS2015512</td>\n",
       "      <td>NITRATE</td>\n",
       "      <td>1/24/2024</td>\n",
       "      <td>8.4</td>\n",
       "      <td>8 - 8.4</td>\n",
       "      <td>ppm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>Runoff from fertilizer use</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CITY OF ABBYVILLE</td>\n",
       "      <td>KS2015512</td>\n",
       "      <td>SELENIUM</td>\n",
       "      <td>1/24/2024</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>ppb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>Erosion of natural deposits</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CITY OF ABILENE</td>\n",
       "      <td>KS2004112</td>\n",
       "      <td>BARIUM</td>\n",
       "      <td>4/16/2024</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.063</td>\n",
       "      <td>ppm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Discharge from metal refineries</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CITY OF ABILENE</td>\n",
       "      <td>KS2004112</td>\n",
       "      <td>FLUORIDE</td>\n",
       "      <td>4/16/2024</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0 - 0.81</td>\n",
       "      <td>ppm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>Natural deposits; Water additive which promote...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CITY OF ABILENE</td>\n",
       "      <td>KS2004112</td>\n",
       "      <td>NITRATE</td>\n",
       "      <td>1/8/2024</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93 - 1</td>\n",
       "      <td>ppm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>Runoff from fertilizer use</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CITY OF ABILENE</td>\n",
       "      <td>KS2004112</td>\n",
       "      <td>SELENIUM</td>\n",
       "      <td>4/16/2024</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>ppb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>Erosion of natural deposits</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CITY OF ADMIRE</td>\n",
       "      <td>KS2011103</td>\n",
       "      <td>BARIUM</td>\n",
       "      <td>4/15/2024</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>ppm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Discharge from metal refineries</td>\n",
       "      <td>2</td>\n",
       "      <td>CITY OF EMPORIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pws name     pws id regulated contaminants collection date  \\\n",
       "0  CITY OF ABBYVILLE  KS2015512                 BARIUM       1/24/2024   \n",
       "1  CITY OF ABBYVILLE  KS2015512               CHROMIUM       1/24/2024   \n",
       "2  CITY OF ABBYVILLE  KS2015512               FLUORIDE       1/24/2024   \n",
       "3  CITY OF ABBYVILLE  KS2015512                NITRATE       1/24/2024   \n",
       "4  CITY OF ABBYVILLE  KS2015512               SELENIUM       1/24/2024   \n",
       "5    CITY OF ABILENE  KS2004112                 BARIUM       4/16/2024   \n",
       "6    CITY OF ABILENE  KS2004112               FLUORIDE       4/16/2024   \n",
       "7    CITY OF ABILENE  KS2004112                NITRATE        1/8/2024   \n",
       "8    CITY OF ABILENE  KS2004112               SELENIUM       4/16/2024   \n",
       "9     CITY OF ADMIRE  KS2011103                 BARIUM       4/15/2024   \n",
       "\n",
       "  highest value range\\n(low/high) unit  mcl mclg  \\\n",
       "0          0.16              0.16  ppm  NaN    2   \n",
       "1           1.4               1.4  ppb  NaN  100   \n",
       "2          0.49              0.49  ppm  NaN    4   \n",
       "3           8.4           8 - 8.4  ppm  NaN   10   \n",
       "4           1.5               1.5  ppb  NaN   50   \n",
       "5         0.063             0.063  ppm  NaN    2   \n",
       "6          0.81          0 - 0.81  ppm  NaN    4   \n",
       "7             1          0.93 - 1  ppm  NaN   10   \n",
       "8           1.8               1.8  ppb  NaN   50   \n",
       "9         0.015             0.015  ppm  NaN    2   \n",
       "\n",
       "                                      typical source          water system  \n",
       "0                    Discharge from metal refineries    2              NaN  \n",
       "1                Discharge from steel and pulp mills  100              NaN  \n",
       "2  Natural deposits; Water additive which promote...    4              NaN  \n",
       "3                         Runoff from fertilizer use   10              NaN  \n",
       "4                        Erosion of natural deposits   50              NaN  \n",
       "5                    Discharge from metal refineries    2              NaN  \n",
       "6  Natural deposits; Water additive which promote...    4              NaN  \n",
       "7                         Runoff from fertilizer use   10              NaN  \n",
       "8                        Erosion of natural deposits   50              NaN  \n",
       "9                    Discharge from metal refineries    2  CITY OF EMPORIA  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Table Extraction for REgulated Contaminants\n",
    "# expected_headers = ['pws name', 'pws id', 'compliance period', 'regulated contaminants', 'collection date', 'highest value', 'range\\n(low/high)','unit','mcl','mclg','typical source']\n",
    "expected_headers = ['pws name', 'pws id', 'regulated contaminants', 'collection date', 'highest value', 'range\\n(low/high)','unit','mcl','mclg','typical source']\n",
    "df = pd.DataFrame(columns=expected_headers)\n",
    "# df.header = expected_headers\n",
    "for test_range in range(3):\n",
    "    doc_path = os.path.join(basedir, subdir, doc_files[test_range])\n",
    "    doc = Document(doc_path)\n",
    "    ###################### Paragraph Extraction #####################\n",
    "    paragraphs = doc.paragraphs #get all the paragraphs in the object\n",
    "    p0 = paragraphs[3]\n",
    "    print(p0)\n",
    "\n",
    "    pws_text = p0.text\n",
    "    regex_search = re.search(\n",
    "    r'pws\\s*name\\s*:\\s*(.*?)\\s+pws\\s*id\\s*:\\s*([A-Za-z0-9\\-]+)',\n",
    "    str(pws_text),\n",
    "    flags=re.IGNORECASE\n",
    "    )\n",
    "    pws_name = regex_search.group(1)\n",
    "    pws_id = regex_search.group(2)\n",
    "        \n",
    "    ###################### Table Extraction #########################\n",
    "    tables = doc.tables\n",
    "    rows_out = [] # setup variable for row data\n",
    "    for i, table in enumerate(tables):\n",
    "        # print(i)\n",
    "        \n",
    "\n",
    "        if tables[i].cell(0,0).text.strip().casefold() == \"regulated contaminants\": #strip removes white sopace and casefold avoids capitalization issues\n",
    "            txt = table.cell(0, 5).text\n",
    "            raw = table.cell(0, 5).text\n",
    "            clean = raw.replace('\\xa0', ' ').strip()\n",
    "            print(\"raw   :\", repr(raw))\n",
    "            print(\"clean :\", repr(clean))\n",
    "\n",
    "            #print([ord(ch) for ch in txt])\n",
    "            # print(repr(table.cell(0, 5).text))\n",
    "            headers = [cell.text.strip().casefold() for cell in table.rows[0].cells] #get all the headers of the current table. We can use these for verification with our pandas df\n",
    "            for r in range(1, len(table.rows)):\n",
    "                row_data = {headers[c]: table.cell(r, c).text.strip() for c in range(len(headers))}\n",
    "                print(row_data)\n",
    "                rows_out.append(row_data)\n",
    "\n",
    "    table_df = pd.DataFrame(rows_out)\n",
    "    table_df[\"pws name\"] = pws_name\n",
    "    table_df[\"pws id\"] = pws_id\n",
    "    # col_map = {h: idx for idx, h in enumerate(headers)} #this maps the headers to a dictionary for lookup like col_map.get(\"Containment\")\n",
    "    # for r, rows in enumerate(tables[i].rows):\n",
    "    #     for c, cell in enumerate(rows.cells):\n",
    "            # print(r, c, cell.text)\n",
    "    # print('table ', i, 'is correct')\n",
    "    df = pd.concat([df, table_df], ignore_index=True)\n",
    "# display(df)\n",
    "# display(row_data)\n",
    "# display(table_df)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "281773cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<docx.text.paragraph.Paragraph object at 0x000002746B208190>\n",
      "PWS NAME:\tCITY OF ADMIRE\t\t\tPWS ID: KS2011103   \n"
     ]
    }
   ],
   "source": [
    "paragraphs = doc.paragraphs #get all the paragraphs in the object\n",
    "p0 = paragraphs[3]\n",
    "print(p0)\n",
    "\n",
    "text = p0.text\n",
    "print(text) # gives us the first paragraph (the title of the document)\n",
    "\n",
    "# tables = doc.tables\n",
    "# t0 = tables[0] #this indexes the first table in the doc\n",
    "# print(t0) # no information\n",
    "\n",
    "# #need to index into the cells for the table data\n",
    "# rows = t0.rows\n",
    "# r0 = rows[0]\n",
    "# print(r0)\n",
    "# cells = r0.cells\n",
    "# c00 = cells[0]\n",
    "# print(c00)\n",
    "# cell_text = c00.text\n",
    "# print(t0.cell(5,0).text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afc3ac88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: C:\\Users\\Joey\\Desktop\\KU Classes\\KDSC\\Boil Water Advisories (Level 3)\\Compliance_Report_PDFs\\Kansas-Annual-Compliance-Report-2023_202406201048536415.pdf\n"
     ]
    }
   ],
   "source": [
    "# If needed, uncomment:\n",
    "# !pip install pymupdf pdfplumber pypdf matplotlib\n",
    "\n",
    "from pathlib import Path\n",
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "from pypdf import PdfReader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PDF_PATH = Path(r\"C:\\Users\\Joey\\Desktop\\KU Classes\\KDSC\\Boil Water Advisories (Level 3)\\Compliance_Report_PDFs\\Kansas-Annual-Compliance-Report-2023_202406201048536415.pdf\")  # <-- change this\n",
    "assert PDF_PATH.exists(), f\"File not found: {PDF_PATH}\"\n",
    "print(\"Using:\", PDF_PATH.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c729b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Basic Info (pypdf) ===\n",
      "Pages: 57\n",
      "Metadata: {'/Author': 'KANSAS DEPARTMNT O HEALTH AND ENVIRONMENT', '/CreationDate': \"D:20240618090351-05'00'\", '/Creator': 'Adobe Acrobat Pro 2017 17.12.30262', '/ModDate': \"D:20240618090351-05'00'\", '/Producer': 'Adobe Acrobat Pro 2017 17.12.30262', '/Title': ''}\n",
      "\n",
      "=== Quick text length per page (first 15 pages) ===\n",
      "Page   1:    264 chars\n",
      "Page   2:   5374 chars\n",
      "Page   3:   4354 chars\n",
      "Page   4:    344 chars\n",
      "Page   5:   3765 chars\n",
      "Page   6:   2483 chars\n",
      "Page   7:   2312 chars\n",
      "Page   8:    778 chars\n",
      "Page   9:   1636 chars\n",
      "Page  10:   2192 chars\n",
      "Page  11:   3167 chars\n",
      "Page  12:   1228 chars\n",
      "Page  13:   1276 chars\n",
      "Page  14:   2303 chars\n",
      "Page  15:   3325 chars\n"
     ]
    }
   ],
   "source": [
    "reader = PdfReader(str(PDF_PATH))\n",
    "print(\"=== Basic Info (pypdf) ===\")\n",
    "print(\"Pages:\", len(reader.pages))\n",
    "print(\"Metadata:\", reader.metadata)\n",
    "\n",
    "print(\"\\n=== Quick text length per page (first 15 pages) ===\")\n",
    "for i, page in enumerate(reader.pages[:15]):\n",
    "    txt = page.extract_text() or \"\"\n",
    "    print(f\"Page {i+1:>3}: {len(txt):>6} chars\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51984278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import fitz\n",
    "\n",
    "PAGE_NUM = 53\n",
    "zoom = 2.0\n",
    "\n",
    "doc = fitz.open(str(PDF_PATH))\n",
    "page = doc[PAGE_NUM - 1]\n",
    "pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom), alpha=False)\n",
    "\n",
    "# Convert pixmap samples -> numpy image array\n",
    "img = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, pix.n)\n",
    "\n",
    "plt.figure(figsize=(10, 12))\n",
    "if pix.n == 1:\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "else:\n",
    "    plt.imshow(img)  # RGB\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"Page {PAGE_NUM} visual layout\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a45e464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 blocks on page 53\n",
      "\n",
      " 1. bbox=(  346.1,   36.3,  445.0,   46.9) type=0 text='2023 BOIL WATER ADVISORIES'\n",
      " 2. bbox=(   43.2,   72.2,  699.8,  105.9) type=0 text='Federal ID System Name Issued Rescinded County District POP Reason Alternative Type/Comments KS2009505 Norwich 1/2/2023 '\n",
      " 3. bbox=(   42.8,  104.7,  726.1,  339.3) type=0 text='Leaking fire hydrant, can't make repairs immediately, shutting off water KS2000708 Sharon 1/5/2023 1/6/2023 Barber SWD 1'\n",
      " 4. bbox=(   43.2,  108.7,  513.3,  121.9) type=0 text='KS2001515 Leon 1/3/2023 1/5/2023 Butler SCD 667 Loss of Pressure'\n",
      " 5. bbox=(   43.2,  338.1,  743.7,  379.4) type=0 text='System upgrades causing loss of pressure, second 75 day bwa sent to system to hand issue. KS2005527 Southwind Subdivisio'\n",
      " 6. bbox=(   43.2,  346.3,  512.6,  355.3) type=0 text='KS2001102 Fulton, City of 6/7/2023 10/12/2023 Bourbon SED 165 Loss of pressure'\n",
      " 7. bbox=(   43.2,  386.3,  741.8,  419.5) type=0 text='KS2007304 Fall River, City of 6/19/2023 6/23/2023 Greenwood SED 129 Loss of pressure Trash truck ran over meter and brok'\n",
      " 8. bbox=(   42.8,  418.3,  729.3,  467.6) type=0 text='watermain break and tower drain. Impacts customers West of HW 69. KS2002113 City of Galena 6/30/2023 7/3/2023 Cherokee S'\n",
      " 9. bbox=(   43.2,  422.2,  512.6,  435.4) type=0 text='KS2010707 Linn RWD 2 7/2/2023 7/6/2023 Linn SED 1960 Loss of pressure'\n",
      "10. bbox=(   43.2,  466.4,  737.1,  499.6) type=0 text='self issued-construction tie in related when connecting to garden plain. KS2001302 Reserve, City of 7/11/2023 7/19/2023 '\n",
      "11. bbox=(   43.2,  470.3,  512.6,  483.5) type=0 text='KS2115502 Cheney State Park Marina 7/10/2023 7/26/2023 Reno SCD 25 Loss of pressure'\n",
      "12. bbox=(   42.8,  502.4,  739.0,  555.8) type=0 text='KS2014916 Walnut Grove MHC Brooks INCORRECT 7/12/2023 7/12/2023 Pottawatomie NED 203 Loss of Pressure had to shut down t'\n"
     ]
    }
   ],
   "source": [
    "PAGE_NUM = 53\n",
    "doc = fitz.open(str(PDF_PATH))\n",
    "page = doc[PAGE_NUM - 1]\n",
    "\n",
    "blocks = page.get_text(\"blocks\")  # (x0, y0, x1, y1, text, block_no, block_type)\n",
    "print(f\"Found {len(blocks)} blocks on page {PAGE_NUM}\\n\")\n",
    "\n",
    "# Sort top-to-bottom, then left-to-right\n",
    "blocks_sorted = sorted(blocks, key=lambda b: (round(b[1], 1), round(b[0], 1)))\n",
    "\n",
    "for idx, b in enumerate(blocks_sorted[:30], 1):  # print first 30 blocks\n",
    "    x0, y0, x1, y1, text, bno, btype = b\n",
    "    snippet = \" \".join((text or \"\").split())[:120]\n",
    "    print(f\"{idx:>2}. bbox=({x0:7.1f},{y0:7.1f},{x1:7.1f},{y1:7.1f}) type={btype} text='{snippet}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a840053e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words on page 1: 33\n",
      "First 40 words with coordinates:\n",
      "\n",
      "(  304.4,  728.7,  307.5,  743.5)  i\n",
      "(  109.9,   47.6,  182.9,   72.5)  KANSAS\n",
      "(  187.4,   47.6,  255.4,   72.5)  PUBLIC\n",
      "(  259.9,   47.6,  327.9,   72.5)  WATER\n",
      "(  332.4,   47.6,  402.5,   72.5)  SUPPLY\n",
      "(  407.0,   47.6,  501.9,   72.5)  PROGRAM\n",
      "(   67.6,   94.3,  195.9,  135.8)  ANNUAL\n",
      "(  203.5,   94.3,  411.8,  135.8)  COMPLIANCE\n",
      "(  419.3,   94.3,  544.3,  135.8)  REPORT\n",
      "(  207.0,  151.0,  308.9,  176.0)  CALENDAR\n",
      "(  313.4,  151.0,  364.5,  176.0)  YEAR\n",
      "(  369.0,  151.0,  405.0,  176.0)  2023\n",
      "(  251.6,  668.3,  276.2,  684.4)  Janet\n",
      "(  279.2,  668.3,  311.9,  684.4)  Stanek\n",
      "(  314.9,  668.3,  360.1,  684.4)  Secretary\n",
      "(  181.2,  682.1,  199.8,  698.2)  Leo\n",
      "(  202.8,  682.1,  214.4,  698.2)  G.\n",
      "(  217.4,  682.1,  261.7,  698.2)  Henning,\n",
      "(  264.7,  682.1,  307.8,  698.2)  Director,\n",
      "(  310.8,  682.1,  352.2,  698.2)  Division\n",
      "(  355.2,  682.1,  365.2,  698.2)  of\n",
      "(  368.2,  682.1,  430.8,  698.2)  Environment\n",
      "(  213.8,  695.9,  236.5,  712.0)  Tom\n",
      "(  239.5,  695.9,  269.3,  712.0)  Stiles,\n",
      "(  272.3,  695.9,  315.1,  712.0)  Director,\n",
      "(  318.1,  695.9,  352.7,  712.0)  Bureau\n",
      "(  355.7,  695.9,  365.7,  712.0)  of\n",
      "(  368.6,  695.9,  398.0,  712.0)  Water\n",
      "(  206.9,  709.7,  235.6,  725.8)  Cathy\n",
      "(  238.6,  709.7,  308.8,  725.8)  Tucker-Vogel,\n",
      "(  311.8,  709.7,  336.4,  725.8)  PWS\n",
      "(  339.5,  709.7,  375.5,  725.8)  Section\n",
      "(  378.5,  709.7,  405.2,  725.8)  Chief\n"
     ]
    }
   ],
   "source": [
    "PAGE_NUM = 1\n",
    "doc = fitz.open(str(PDF_PATH))\n",
    "page = doc[PAGE_NUM - 1]\n",
    "\n",
    "words = page.get_text(\"words\")  \n",
    "# format: (x0, y0, x1, y1, \"word\", block_no, line_no, word_no)\n",
    "\n",
    "print(f\"Total words on page {PAGE_NUM}: {len(words)}\")\n",
    "print(\"First 40 words with coordinates:\\n\")\n",
    "for w in words[:40]:\n",
    "    x0, y0, x1, y1, word, block_no, line_no, word_no = w\n",
    "    print(f\"({x0:7.1f},{y0:7.1f},{x1:7.1f},{y1:7.1f})  {word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "575ac308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser as dateparser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bfbced",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LIST_URL = \"https://www.kdhe.ks.gov/m/newsflash?cat=29\"\n",
    "BASE_URL = \"https://www.kdhe.ks.gov\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; kdhe-newsflash-scraper/1.0; +https://example.com/)\"\n",
    "}\n",
    "\n",
    "DETAIL_HREF_RE = re.compile(r\"^/m/newsflash/Home/Detail/\\d+\")\n",
    "\n",
    "\n",
    "def fetch_soup(session: requests.Session, url: str) -> BeautifulSoup:\n",
    "    resp = session.get(url, headers=HEADERS, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    return BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "\n",
    "def extract_detail_links(list_soup: BeautifulSoup) -> list[str]:\n",
    "    links = []\n",
    "    for a in list_soup.select(\"a[href]\"):\n",
    "        href = a.get(\"href\", \"\").strip()\n",
    "        if DETAIL_HREF_RE.match(href):\n",
    "            links.append(urljoin(BASE_URL, href))\n",
    "    # de-dupe while preserving order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for u in links:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            out.append(u)\n",
    "    return out\n",
    "\n",
    "\n",
    "def parse_detail_page(detail_soup: BeautifulSoup, url: str) -> dict:\n",
    "    # Title: on these pages there can be more than one H1 (\"News Flash\" + the actual item title),\n",
    "    # so taking the LAST H1 is usually the article title. :contentReference[oaicite:1]{index=1}\n",
    "    h1s = detail_soup.find_all(\"h1\")\n",
    "    title = h1s[-1].get_text(\" \", strip=True) if h1s else None\n",
    "\n",
    "    # Find the line that contains \"Posted on ...\"\n",
    "    text_lines = [ln.strip() for ln in detail_soup.get_text(\"\\n\").splitlines() if ln.strip()]\n",
    "    meta_line = next((ln for ln in text_lines if \"Posted on\" in ln), \"\")\n",
    "\n",
    "    # Example meta line looks like:\n",
    "    # \"Press Releases   Posted on January 30, 2026 | Last Updated on January 30, 2026\" :contentReference[oaicite:2]{index=2}\n",
    "    category = None\n",
    "    posted_dt = None\n",
    "    updated_dt = None\n",
    "\n",
    "    if meta_line:\n",
    "        category = meta_line.split(\"Posted on\")[0].strip() or None\n",
    "\n",
    "        m_posted = re.search(r\"Posted on\\s+([^|]+)\", meta_line)\n",
    "        if m_posted:\n",
    "            posted_dt = dateparser.parse(m_posted.group(1).strip())\n",
    "\n",
    "        m_updated = re.search(r\"Last Updated on\\s+(.+)$\", meta_line)\n",
    "        if m_updated:\n",
    "            updated_dt = dateparser.parse(m_updated.group(1).strip())\n",
    "\n",
    "    # Body: collect text after the title until \"Related News\" (which appears on detail pages). :contentReference[oaicite:3]{index=3}\n",
    "    body = \"\"\n",
    "    title_tag = h1s[-1] if h1s else None\n",
    "    if title_tag:\n",
    "        chunks = []\n",
    "        for sib in title_tag.next_siblings:\n",
    "            if not hasattr(sib, \"get_text\"):\n",
    "                continue\n",
    "            t = sib.get_text(\" \", strip=True)\n",
    "            if not t:\n",
    "                continue\n",
    "            if \"Related News\" in t:\n",
    "                break\n",
    "            # Skip repeating the meta line if it gets picked up\n",
    "            if \"Posted on\" in t and \"Last Updated\" in t:\n",
    "                continue\n",
    "            chunks.append(t)\n",
    "        body = \"\\n\".join(chunks).strip()\n",
    "\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"title\": title,\n",
    "        \"category\": category,\n",
    "        \"posted\": posted_dt,\n",
    "        \"last_updated\": updated_dt,\n",
    "        \"body\": body,\n",
    "    }\n",
    "\n",
    "\n",
    "def scrape_kdhe_newsflash(max_items: int = 25, sleep_s: float = 0.5) -> pd.DataFrame:\n",
    "    with requests.Session() as session:\n",
    "        list_soup = fetch_soup(session, LIST_URL)\n",
    "        detail_urls = extract_detail_links(list_soup)\n",
    "\n",
    "        rows = []\n",
    "        for u in detail_urls[:max_items]:\n",
    "            detail_soup = fetch_soup(session, u)\n",
    "            rows.append(parse_detail_page(detail_soup, u))\n",
    "            time.sleep(sleep_s)  # be polite\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    # Optional: consistent ordering\n",
    "    df = df.sort_values([\"posted\", \"title\"], ascending=[False, True], na_position=\"last\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = scrape_kdhe_newsflash(max_items=20)\n",
    "    print(df[[\"posted\", \"title\", \"url\"]].head(10))\n",
    "    df.to_csv(\"kdhe_boil_water_advisories.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscrape-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
